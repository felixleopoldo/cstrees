#!python3
import contextlib
from functools import partial, reduce
from importlib.metadata import version
from itertools import pairwise, product
import operator
import os
from pathlib import Path
import random
import time
import warnings

old_environ = dict(os.environ)
os.environ.update({"TQDM_DISABLE": "1", "TQDM_ENABLE": "0", "NUMEXPR_MAX_THREADS": "8"})

import causaldag as cd
from causallearn.search.ConstraintBased.PC import pc
from causallearn.search.PermutationBased.GRaSP import grasp
import numpy as np
import matplotlib as mpl
from p_tqdm import t_map  # p_umap
import pandas as pd
from pgmpy.models import BayesianNetwork
from scipy.special import rel_entr
import seaborn as sns

from cstrees import cstree as ct
from cstrees.evaluate import kl_divergence
import cstrees.learning as ctl
import cstrees.scoring as sc


def reset_seed():
    np.random.seed(1312)
    random.seed(1312)


def learn_cstree(data_path):
    # load data
    data = pd.read_csv(data_path, sep=" ", header=None).iloc[:, :-1]
    data = pd.DataFrame(
        pd.concat([data.nunique().to_frame().T.astype(int), data]).to_numpy()
    )

    # estimate possible context variables and create score tables
    grasp_graph = grasp(data.values[1:], score_func="local_score_BDeu", maxP=5, depth=3)
    # pcgraph = pc(data.values, 0.001, "gsq", node_names=data.columns)
    poss_cvars = ctl.causallearn_graph_to_posscvars(
        grasp_graph, labels=data.columns, alg="grasp"
    )
    score_table, context_scores, _ = sc.order_score_tables(
        data, max_cvars=2, alpha_tot=1.0, method="BDeu", poss_cvars=poss_cvars
    )

    # run Gibbs sampler to get MAP order
    orders, scores = ctl.gibbs_order_sampler(5000, score_table)
    map_order = orders[scores.index(max(scores))]

    # estimate CStree
    opt_tree = ctl._optimal_cstree_given_order(map_order, context_scores)

    return opt_tree


def learn_synth_cstree(data, return_pc=False):
    # estimate possible context variables and create score tables
    pcgraph = pc(data.values[1:], 0.05, "gsq", node_names=data.columns)
    poss_cvars = ctl.causallearn_graph_to_posscvars(pcgraph, labels=data.columns)

    score_table, context_scores, _ = sc.order_score_tables(
        data, max_cvars=2, alpha_tot=1.0, method="BDeu", poss_cvars=poss_cvars
    )

    # run Gibbs sampler to get MAP order
    orders, scores = ctl.gibbs_order_sampler(5000, score_table)
    map_order = orders[scores.index(max(scores))]

    # estimate CStree
    opt_tree = ctl._optimal_cstree_given_order(map_order, context_scores)

    return opt_tree if not return_pc else (opt_tree, pcgraph)


def kl_exp(cards, samp_sizes):
    num_levels = len(cards)
    true = ct.sample_cstree(cards, max_cvars=2, prob_cvar=0.5, prop_nonsingleton=1)
    true.sample_stage_parameters(alpha=2)

    est_list = []
    for samp_size in samp_sizes:
        data = true.sample(samp_size)
        # ts = time.time()
        # data.to_csv(f"{path}dataset-{ts}.csv")
        if samp_size == 10000:
            est, pcgraph = learn_synth_cstree(data, return_pc=True)
            # fit pgmpy bayesian network and compute its kl divergence
            edge_list = pcgraph.find_adj()
            adj_mat = np.zeros((num_levels, num_levels), bool)
            adj_mat[tuple(zip(*edge_list))] = True
            cpdag = cd.PDAG.from_amat(adj_mat)  # circumvent bug in pgmpy
            dag = cpdag.to_dag().to_nx()
            dag.add_nodes_from(range(num_levels))  # circumvent bug in cd
            pgm = BayesianNetwork(dag)
            pgm.fit(data[1:])
            dag_kl = kl_div_pc(pgm, true)

        else:
            est = learn_synth_cstree(data)
        est.estimate_stage_parameters(data)
        est._create_tree()
        est_list.append(kl_divergence(est, true))

    # generate random cstree and compute its kl divergence
    rand = ct.sample_cstree(cards, max_cvars=3, prob_cvar=0.5, prop_nonsingleton=1)
    rand.sample_stage_parameters(alpha=2)
    rand.sample(100)
    rand_kl = kl_divergence(rand, true)

    return [dag_kl] + [rand_kl] + est_list


def run_kl_experiments(path):
    reset_seed()

    samp_size_range = (10000, 250, 500, 1000)
    num_runs_per = 10
    num_levels_range = range(5, 21, 5)

    intermed_path = f"{path}kl_intermediate/"
    Path(intermed_path).mkdir(parents=True, exist_ok=True)

    for num_levels in num_levels_range:
        cards = [2] * num_levels

        for run_idx in range(num_runs_per):
            print(f"Starting run {run_idx} for p={num_levels}...")
            update = pd.DataFrame(columns=["p", "samp_size", "KL-divergence"])
            while True:
                try:
                    est_list = kl_exp(cards, samp_size_range)
                    break
                except ValueError:
                    pass
            for samp_size, est_kl in zip([-1, 0] + list(samp_size_range), est_list):
                inner_update = pd.DataFrame(
                    {
                        "p": num_levels,
                        "samp_size": samp_size,
                        "KL-divergence": est_kl,
                    },
                    index=[0],
                )
                update = pd.concat((update, inner_update), ignore_index=True)
            update.to_csv(f"{intermed_path}p={num_levels}_run={run_idx}.tar.gz")

    intermediate_results = {
        (num_levels, run_idx): pd.read_csv(
            f"{intermed_path}p={num_levels}_run={run_idx}.tar.gz",
            index_col=0,
        )
        for run_idx in range(num_runs_per)
        for num_levels in num_levels_range
    }
    results = pd.concat(intermediate_results.values())
    results.to_csv(f"{path}kl_results.tar.gz")


def kl_div_pc(pgm, true):
    # enumerate outcomes
    factorized_outcomes = (range(card) for card in true.cards)
    outcomes = product(*factorized_outcomes)

    # func for outcome-wise relative entropy
    def _rel_entr_of_outcome(outcome):
        # compute true prob of outcome from CStree
        nodes = (outcome[:idx] for idx in range(true.p + 1))
        edges = pairwise(nodes)

        def _probs_map(edge):
            try:
                tru = true.tree[edge[0]][edge[1]]["cond_prob"]
            except KeyError:
                stage = true.get_stage(edge[0])
                tru = stage.probs[edge[1][-1]]
            return tru

        true_prob_outcome = reduce(operator.mul, map(_probs_map, edges))

        # compute est prob of outcome from DAG
        pgmpy_outcome = {idx: marg_outcome for idx, marg_outcome in enumerate(outcome)}
        est_prob_outcome = pgm.get_state_probability(pgmpy_outcome)

        return rel_entr(est_prob_outcome, true_prob_outcome)

    # return KL divergence by summing rel entr of all outcomes
    return sum(map(_rel_entr_of_outcome, outcomes))


def simulate_sparse_data(
    num_levels: int, samp_size: int, max_poss_pa: int = 10, return_time: bool = False
) -> tuple[ct.CStree, pd.DataFrame, dict, float]:
    """Use rejection sampling to generate ground truth with specified sparsity."""
    failed = True
    num_rejected = 0
    while failed:
        print(f"\tNumber of datasets rejected for lack of sparsity: {num_rejected}")
        num_rejected += 1
        # propose ground truth
        cards = [2] * num_levels
        true = ct.sample_cstree(cards, max_cvars=2, prob_cvar=0.5, prop_nonsingleton=1)
        true.sample_stage_parameters(alpha=2)
        data = true.sample(samp_size)

        # check max_poss_pa using PC algorithm
        pc_start = time.time()
        print("\t\tstarting PC")
        pcgraph = pc(data.values[1:], 0.05, "gsq", node_names=data.columns)
        poss_cvars = ctl.causallearn_graph_to_posscvars(pcgraph, labels=data.columns)
        pc_end = time.time()
        pc_time = pc_end - pc_start

        failed = any((len(poss_pa) > max_poss_pa for poss_pa in poss_cvars.values()))

    return (
        (true, data, poss_cvars, pc_time) if return_time else (true, data, poss_cvars)
    )


def runtime_experiment(num_levels, samp_size, max_poss_pa=10):
    _, data, poss_cvars, pc_time = simulate_sparse_data(
        num_levels, samp_size, max_poss_pa, True
    )

    cst_start = time.time()
    score_table, context_scores, _ = sc.order_score_tables(
        data, max_cvars=2, alpha_tot=1.0, method="BDeu", poss_cvars=poss_cvars
    )

    # run Gibbs sampler to get MAP order
    orders, scores = ctl.gibbs_order_sampler(5000, score_table)
    map_order = orders[scores.index(max(scores))]

    # estimate CStree
    _ = ctl._optimal_cstree_given_order(map_order, context_scores)
    cst_end = time.time()
    cst_time = cst_end - cst_start

    return pc_time, cst_time


def run_runtime_experiments(
    path,
    samp_size,
    num_runs,
    num_levels,
):
    print(f"Starting p={num_levels}...")
    samp_size_iter = (samp_size,) * 10
    run_runs = partial(runtime_experiment, num_levels)
    runtimes = t_map(run_runs, samp_size_iter)
    # can use p_uimap instead, with sufficient memory

    pc_runtimes = np.empty(num_runs, float)
    cst_runtimes = np.empty_like(pc_runtimes)
    for run_idx in range(num_runs):
        # pc_runtime, cst_runtime = runtime_experiment(num_levels, samp_size)
        pc_runtime, cst_runtime = runtimes[run_idx]
        pc_runtimes[run_idx] = pc_runtime
        cst_runtimes[run_idx] = cst_runtime
    print(f"...finishing p={num_levels}.")
    total_runtimes = pc_runtimes + cst_runtimes
    runtimes = np.concatenate((pc_runtimes, cst_runtimes, total_runtimes))
    alg = np.repeat(["PC", "CStree", "Total"], 10)
    results = pd.DataFrame(
        {
            "p": num_levels,
            "samp_size": samp_size,
            "runtime": runtimes,
            "alg": alg,
        }
    )
    results.to_csv(f"{path}p={num_levels}_n={samp_size}.tar.gz")


def runtime(path, samp_size, num_levels_range):
    reset_seed()

    num_runs_per = 10
    intermed_path = f"{path}runtime_intermediate/"
    Path(intermed_path).mkdir(parents=True, exist_ok=True)

    run_runtime = partial(
        run_runtime_experiments, intermed_path, samp_size, num_runs_per
    )
    t_map(run_runtime, num_levels_range)
    # can use p_umap instead, with sufficient memory

    intermediate_results = {
        num_levels: pd.read_csv(
            f"{intermed_path}p={num_levels}_n={samp_size}.tar.gz",
            index_col=0,
        )
        for num_levels in num_levels_range
    }
    results = pd.concat(intermediate_results.values())
    results.to_csv(f"{path}runtime_results_n={samp_size}.tar.gz")


def plot_runtime_exps(path):
    for n in (1000, 10000):
        results = pd.read_csv(f"{path}runtime_results_n={n}.tar.gz", index_col=0)

        sns.set(font_scale=1.25)
        sns.set_style("white")
        sns.set_style({"legend.frameon": False})
        g = sns.boxplot(data=results, x="p", y="runtime", hue="alg")

        g.set(
            title=f"",
            xlabel="number of variables ($p$)",
            ylabel="runtime (seconds)",
        )

        g.legend_.set_title("")
        g.legend_.set_frame_on(False)
        new_labels = [
            "constraint-based phase (PC)",
            "Gibbs sampler and exact search",
            "total",
        ]
        for t, l in zip(g.legend_.texts, new_labels):
            t.set_text(l)

        g.figure.tight_layout()
        g.figure.savefig(f"{path}runtime_results_n={n}.pdf")
        g.clear()


def plot_kl_exps(path):
    results = pd.read_csv(f"{path}kl_results.tar.gz", index_col=0)
    samp_sizes = (250, 500, 1000, 10000)

    sns.set(font_scale=1.25)
    sns.set_style("white")
    sns.set_style({"legend.frameon": False})
    continuous_cmap = mpl.colormaps["flare"]
    palette = {
        samp_size: continuous_cmap(c)
        for samp_size, c in zip(samp_sizes, np.linspace(0.2, 0.8, 4))
    }
    discrete_cmap = mpl.colormaps["Set2"]
    palette[-1] = discrete_cmap(2)
    palette[0] = discrete_cmap(4)
    g = sns.boxplot(
        data=results, x="p", y="KL-divergence", hue="samp_size", palette=palette
    )

    g.set(
        xlabel="number of variables ($p$)",
        ylabel="KL-divergence from ground truth",
    )

    g.legend_.set_title("")
    g.legend_.set_frame_on(False)
    new_labels = [
        "PC-estimated DAG MLE for n=10000",
        "random CStree",
        "$\hat \mathcal{T}$ for n=250",
        "$\hat \mathcal{T}$ for n=500",
        "$\hat \mathcal{T}$ for n=1000",
        "$\hat \mathcal{T}$ for n=10000",
    ]
    for t, l in zip(g.legend_.texts, new_labels):
        t.set_text(l)

    g.figure.tight_layout()
    g.figure.savefig(f"{path}kl_results.pdf")
    g.clear()


if __name__ == "__main__":
    # check versions to ensure accurate reproduction
    if version("cstrees") != "1.2.0":
        warnings.warn(f"Current `cstrees` version unsupported.")

    path = "reproduced_uai_results/"
    warnings.simplefilter(action="ignore", category=FutureWarning)

    runtime(path, samp_size=10000, num_levels_range=(10, 25, 50, 100))
    runtime(path, samp_size=1000, num_levels_range=(10, 25, 50, 100, 250, 500))
    plot_runtime_exps(path)

    run_kl_experiments(path)
    plot_kl_exps(path)

    os.environ.clear()
    os.environ.update(old_environ)
